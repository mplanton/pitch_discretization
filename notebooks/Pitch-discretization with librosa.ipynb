{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitch-discretization with librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load libs and input signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define default samplerate of 44100Hz and not 22050Hz\n",
    "# and fft length and hop size\n",
    "from presets import Preset\n",
    "import librosa as _librosa\n",
    "import librosa.display as _display\n",
    "_librosa.display = _display\n",
    "librosa = Preset(_librosa)\n",
    "\n",
    "librosa['sr'] = 44100\n",
    "librosa['n_fft'] = 4096\n",
    "librosa_hop_len = 2048\n",
    "librosa['hop_length'] = librosa_hop_len\n",
    "\n",
    "# other needed modules\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "#x, sr = librosa.load('../../samples/Toms_diner.wav')\n",
    "#x, sr = librosa.load('../../samples/sweep_20Hz_20kHz_10s.wav')\n",
    "\n",
    "#pos = 5\n",
    "#dur = 10\n",
    "#x, sr = librosa.load(\"../../samples/ave-maria.wav\", offset=pos, duration=dur)\n",
    "\n",
    "\n",
    "dur = 10\n",
    "fmin = 500\n",
    "fmax = 4*500\n",
    "sr = 44100\n",
    "x = librosa.chirp(fmin, fmax, sr=sr, duration=dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(sig, sr, title=''):\n",
    "    \"\"\"Plot waveform and spectrogram provide audio player of signal sig with ramplerate sr.\"\"\"\n",
    "    plt.rcParams['figure.figsize'] = [15, 4]\n",
    "    t = np.linspace(0, sig.size/sr, sig.size)\n",
    "    plt.plot(t, sig)\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [15, 8]\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(sig)), ref=np.max)\n",
    "    fig, ax = plt.subplots()\n",
    "    img = librosa.display.specshow(D, x_axis='time', y_axis='log', ax=ax)\n",
    "    ax.set(title=title)\n",
    "    fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "\n",
    "my_plot(x, sr, \"original\")\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pitch-tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "fmin = 80\n",
    "fmax = 2100\n",
    "frame_len = 2048 # default is 2048\n",
    "\n",
    "f0, voiced_flag, voiced_probs = librosa.pyin(x, fmin=fmin, fmax=fmax, sr=sr, frame_length=frame_len)\n",
    "\n",
    "times = librosa.times_like(f0)\n",
    "\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(x)), ref=np.max)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(D, x_axis='time', y_axis='log', ax=ax)\n",
    "ax.set(title='pYIN fundamental frequency estimation')\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "ax.plot(times, f0, label='f0', color='cyan', linewidth=3)\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for windowed block processing\n",
    "blocksize = librosa_hop_len\n",
    "n_blocks = f0.size - 1 # avoid last block of some length\n",
    "\n",
    "w = np.hanning(blocksize)\n",
    "\n",
    "y = []\n",
    "# loop over blocks\n",
    "for i in range(n_blocks):\n",
    "    pitch = f0[i]\n",
    "    if np.isnan(pitch):\n",
    "        # unvoiced\n",
    "        shifted = x[i*blocksize:(i+1)*blocksize] * w # unprocessed\n",
    "    else:\n",
    "        shifted = x[i*blocksize:(i+1)*blocksize] * w\n",
    "    y.append(shifted)\n",
    "y = np.concatenate(y)\n",
    "\n",
    "my_plot(y, sr, \"block processed spectrum\")\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pitch-shifting to constant pitch\n",
    "\n",
    "We get an\n",
    "- audio signal\n",
    "- the analyzed pitch signal\n",
    "- the pitch analysis hop length (librosa_hop_len)\n",
    "- the wanted pitch\n",
    "\n",
    "Now we want to generate a signal with constant pitch, so the pitch shifting factor depends on the pitch of the input signal.\n",
    "\n",
    "- Figure the blocksize out\n",
    "- calculate the pitch shifting factor for each block\n",
    "- do pitch shifting on each block (and do nothing for unvoiced frames or stay constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = 200 # desired pitch of the output signal\n",
    "\n",
    "semitone_ratio = 2**(1/12)\n",
    "\n",
    "blocksize = librosa_hop_len\n",
    "n_blocks = f0.size - 1 # avoid last block of some length\n",
    "\n",
    "w = np.hanning(blocksize)\n",
    "\n",
    "y_const = []\n",
    "# loop over blocks\n",
    "for i in range(n_blocks):\n",
    "    pitch = f0[i]\n",
    "    if np.isnan(pitch):\n",
    "        # unvoiced\n",
    "        shifted = x[i*blocksize:(i+1)*blocksize] * w # unprocessed\n",
    "    else:\n",
    "        # pitch-shifting ratio rho\n",
    "        rho = f_out / pitch\n",
    "        # to semitones\n",
    "        semitones = 12*np.log2(rho)\n",
    "        # pitch shift\n",
    "        shifted = librosa.effects.pitch_shift(x[i*blocksize:(i+1)*blocksize], n_steps=semitones) * w\n",
    "    y_const.append(shifted)\n",
    "y_const = np.concatenate(y_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot(y_const, sr, \"spectrum pitch-shifting to constant pitch\")\n",
    "ipd.Audio(y_const, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appearing problems\n",
    "\n",
    "* Discontinuities if no windowing is applied, because the pitch-shifting algorithm is not for small block processing but rather for shifting a big chunk of continuous audio data.\n",
    "\n",
    "* The blocksize can not be smaller than 2048 samples because the phase vocoder is hard coded to this fft-size.\n",
    "\n",
    "So make your own OLA-pitch-shifter for the simplest implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(f0, n_tones=12, tune=440):\n",
    "    \"\"\"\n",
    "    nonlinear frequency scale\n",
    "    f0...input frequency\n",
    "    n_tones...make a scale with n_tones tones\n",
    "    tune...tuning frequency\n",
    "    return discrete frequencies\n",
    "    \"\"\"\n",
    "    tone = n_tones * np.log2(f0/tune)\n",
    "    discrete = np.round(tone)\n",
    "    return tune * (2 ** (discrete / n_tones))\n",
    "\n",
    "# test scale\n",
    "f_test = np.linspace(60, 1000, 500)\n",
    "f_disc = scale(f_test)\n",
    "\n",
    "plt.plot(f_test)\n",
    "plt.plot(f_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromatic = 12\n",
    "wholetone = 6\n",
    "\n",
    "# choose scale\n",
    "s = chromatic\n",
    "\n",
    "blocksize = librosa_hop_len\n",
    "n_blocks = f0.size - 1\n",
    "w = np.hanning(blocksize)\n",
    "y_disc = []\n",
    "# loop over blocks\n",
    "for i in range(n_blocks):\n",
    "    pitch = f0[i]\n",
    "    if np.isnan(pitch):\n",
    "        # unvoiced\n",
    "        shifted = x[i*blocksize:(i+1)*blocksize] * w # unprocessed\n",
    "    else:\n",
    "        f_out = scale(pitch, s)\n",
    "        # pitch-shifting ratio rho\n",
    "        rho = f_out / pitch\n",
    "        # to semitones\n",
    "        semitones = 12*np.log2(rho)\n",
    "        # pitch shift\n",
    "        shifted = librosa.effects.pitch_shift(x[i*blocksize:(i+1)*blocksize], n_steps=semitones) * w\n",
    "    y_disc.append(shifted)\n",
    "y_disc = np.concatenate(y_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot(y_disc, sr, \"spectrum pitch-shifting to discrete pitch\")\n",
    "ipd.Audio(y_disc, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appearing problems\n",
    "\n",
    "Amplitude modulation from the windowing appears as the strongest artifact.\n",
    "This is because there is no overlap of the blocks in the block processing.\n",
    "This has to be done inside the pitch shifting algorithm, so program your own implementation and check the results with overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch discretization algorithm\n",
    "\n",
    "For analyzing every processing step, every step is done for the whole signal.\n",
    "Later for the efficient implementation, this is all done in block processing.\n",
    "\n",
    "Which functions do we need?\n",
    "\n",
    "On the top level, there is the `reshift()` function, which does the pitch discretization for a whole signal.\n",
    "So `reshift()` does the pitch analysis and the pitch shifting.\n",
    "Pitch analysis is done with the librosa implementation of *pYin* for the whole signal.\n",
    "Then the pitch-shifting factor $\\rho[n]$ is calculated for the whole signal.\n",
    "Now the function `pitch_shift()` does the pitch-shifting according to the pitch-shifting factor.\n",
    "In this function we can try different algorithms.\n",
    "\n",
    "1. As a first start, we implement the method from above, which worked OK as a proof of concept.\n",
    "\n",
    "2. Then we implement pitch-shifting via OLA, so we can have overlaps and its a nice to have comparison.\n",
    "\n",
    "3. Then we implement the 'Rollers' algorithm for the efficient implementation\n",
    "\n",
    "4. Then we do testing and check all parameters\n",
    "\n",
    "\n",
    "## pitch-shift OLA\n",
    "\n",
    "This method takes\n",
    "\n",
    "- the audio signal\n",
    "- the analyzed pitch signal\n",
    "- the pitch analysis hop length (librosa_hop_len)\n",
    "- and the wanted scale\n",
    "\n",
    "and outputs the pitch-shifted signal\n",
    "(maybe employ some smoothing of pitch transitions?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
